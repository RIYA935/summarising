{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utn2HnRwrPsb",
        "outputId": "1080d824-d3ad-445c-ef99-2aa67abf207b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 9.8841\n",
            "Epoch [20/100], Loss: 0.5386\n",
            "Epoch [30/100], Loss: 0.1732\n",
            "Epoch [40/100], Loss: 0.1101\n",
            "Epoch [50/100], Loss: 0.0812\n",
            "Epoch [60/100], Loss: 0.0639\n",
            "Epoch [70/100], Loss: 0.0524\n",
            "Epoch [80/100], Loss: 0.0443\n",
            "Epoch [90/100], Loss: 0.0384\n",
            "Epoch [100/100], Loss: 0.0339\n",
            "Laugh at me. thatâ€™s how!\n",
            "Sex offenders live around you?!\n",
            "Lot of bad jobs. hell,!\n",
            "Like, i know, it sounds!\n",
            "Guys have any kids? how!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Step 1: Prepare the dataset\n",
        "# You need to provide a list of comedy routine sentences as your training data.\n",
        "# For demonstration purposes, we'll use a small example dataset.\n",
        "\n",
        "training_data5 = open('/content/AJ_TP_text_05.txt', 'r').read().splitlines()\n",
        "training_data2 = open('/content/AJ_TP_text_02.txt', 'r').read().splitlines()\n",
        "training_data3 = open('/content/AJ_TP_text_03.txt', 'r').read().splitlines()\n",
        "training_data4 = open('/content/AJ_TP_text_04.txt', 'r').read().splitlines()\n",
        "training_data6 = open('/content/AJ_TP_text_06.txt', 'r').read().splitlines()\n",
        "training_data7 = open('/content/AJ_TP_text_07.txt', 'r').read().splitlines()\n",
        "training_data = open('/content/AJ_TP_text_01.txt', 'r').read().splitlines()\n",
        "for i in training_data2 :\n",
        "    training_data.append(i)\n",
        "\n",
        "for i in training_data3 :\n",
        "    training_data.append(i)\n",
        "\n",
        "for i in training_data4 :\n",
        "    training_data.append(i)\n",
        "\n",
        "for i in training_data5 :\n",
        "    training_data.append(i)\n",
        "\n",
        "for i in training_data6 :\n",
        "    training_data.append(i)\n",
        "for i in training_data7 :\n",
        "    training_data.append(i)\n",
        "# Step 2: Tokenize the data\n",
        "# We'll convert the text into a list of words (tokens).\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "tokenized_data = [tokenize(sentence.lower()) for sentence in training_data]\n",
        "\n",
        "# Step 3: Create word-to-index and index-to-word dictionaries\n",
        "\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "for sentence in tokenized_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_index:\n",
        "            index = len(word_to_index)\n",
        "            word_to_index[word] = index\n",
        "            index_to_word[index] = word\n",
        "\n",
        "# Add special tokens for padding and end-of-sentence (EOS)\n",
        "pad_token = '<PAD>'\n",
        "eos_token = '<EOS>'\n",
        "word_to_index[pad_token] = len(word_to_index)\n",
        "word_to_index[eos_token] = len(word_to_index)\n",
        "index_to_word[len(word_to_index) - 2] = pad_token\n",
        "index_to_word[len(word_to_index)-1 ] = eos_token\n",
        "\n",
        "# Step 4: Convert text sequences to numerical sequences\n",
        "\n",
        "def text_to_sequence(text):\n",
        "    return [word_to_index[word] for word in text] + [word_to_index[eos_token]]\n",
        "\n",
        "numerical_data = [text_to_sequence(sentence) for sentence in tokenized_data]\n",
        "\n",
        "# Step 5: Prepare the data for training\n",
        "\n",
        "def create_batches(data, batch_size):\n",
        "    random.shuffle(data)\n",
        "    num_batches = len(data) // batch_size\n",
        "    batches = [data[i * batch_size: (i + 1) * batch_size] for i in range(num_batches)]\n",
        "    return batches\n",
        "\n",
        "batch_size = 2\n",
        "batches = create_batches(numerical_data, batch_size)\n",
        "\n",
        "# Step 6: Define the LSTM Language Model\n",
        "\n",
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTMLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Step 7: Training the LSTM Language Model\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(word_to_index)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 100\n",
        "learning_rate = 0.01\n",
        "num_epochs = 100\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model and move to the device\n",
        "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in batches:\n",
        "        input_seqs = [torch.tensor(seq[:-1]).unsqueeze(0).to(device) for seq in batch]\n",
        "        target_seqs = [torch.tensor(seq[1:]).unsqueeze(0).to(device) for seq in batch]\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            output = model(input_seqs[i])\n",
        "            loss += loss_function(output.view(-1, vocab_size), target_seqs[i].view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Step 8: Generate comedy routines using the trained model\n",
        "\n",
        "def generate_comedy_routine(model, max_length=5):\n",
        "    model.eval()\n",
        "    start_word = random.choice(list(word_to_index.keys()))\n",
        "    input_seq = torch.tensor([word_to_index[start_word]]).unsqueeze(0).to(device)\n",
        "    generated_routine = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            output = model(input_seq)\n",
        "            _, predicted_index = torch.max(output[:, -1, :], dim=1)\n",
        "            predicted_word = index_to_word[predicted_index.item()]\n",
        "            if predicted_word == eos_token:\n",
        "                break\n",
        "            generated_routine.append(predicted_word)\n",
        "            input_seq = torch.cat((input_seq, predicted_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return ' '.join(generated_routine)\n",
        "\n",
        "# Generate comedy routines\n",
        "for _ in range(5):\n",
        "    routine = generate_comedy_routine(model, max_length=5)\n",
        "    print(routine.capitalize() + '!')"
      ]
    }
  ]
}